{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import librosa.display\n",
    "\n",
    "def showimage(path):\n",
    "    img = mpimg.imread(path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_audio(rep,path):\n",
    "    plt.axis('off')\n",
    "    if rep =='a1':\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        librosa.display.waveplot(x, sr=sr)\n",
    "        #print('a1:waveplot')\n",
    "    else:\n",
    "        img = mpimg.imread(path)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "def get_specfilename(name,folder):\n",
    "    name = name.split('.')[0]\n",
    "    name =  name.split('/')[1].split('_')\n",
    "    name = folder+'_'.join(name)+'.png'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://drive.google.com/file/d/1n2iPBxM6FqxIrIrk2EXwgRzMGBDjAAym/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing tensorboard\n",
    "#!conda install -c conda-forge tensorboard -y\n",
    "#!conda install -c anaconda tensorboard -y\n",
    "!conda install -c conda-forge tensorflow -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Fashion MNIST with PyTorch and Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard PyTorch modules\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter # TensorBoard support\n",
    "\n",
    "# import torchvision module to handle image manipulation\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# calculate train time, writing train data to files etc.\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)     # On by default, leave it here for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check PyTorch versions\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use standard FashionMNIST dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()                                 \n",
    "    ])\n",
    ")\n",
    "val_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()                                 \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network, expand on top of nn.Module\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # define layers\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "    self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "    self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "    self.out = nn.Linear(in_features=60, out_features=51)\n",
    "\n",
    "  # define forward function\n",
    "  def forward(self, t):\n",
    "    # conv 1\n",
    "    t = self.conv1(t)\n",
    "    t = F.relu(t)\n",
    "    t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # conv 2\n",
    "    t = self.conv2(t)\n",
    "    t = F.relu(t)\n",
    "    t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # fc1\n",
    "    t = t.reshape(-1, 12*4*4)\n",
    "    t = self.fc1(t)\n",
    "    t = F.relu(t)\n",
    "\n",
    "    # fc2\n",
    "    t = self.fc2(t)\n",
    "    t = F.relu(t)\n",
    "\n",
    "    # output\n",
    "    t = self.out(t)\n",
    "    # don't need softmax here since we'll use cross-entropy as activation.\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules to build RunBuilder and RunManager helper classes\n",
    "from collections  import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "\n",
    "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
    "# combinations of hyper-parameters\n",
    "class RunBuilder():\n",
    "  @staticmethod\n",
    "  def get_runs(params):\n",
    "\n",
    "    Run = namedtuple('Run', params.keys())\n",
    "\n",
    "    runs = []\n",
    "    for v in product(*params.values()):\n",
    "      runs.append(Run(*v))\n",
    "    \n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class, help track loss, accuracy, epoch time, run time, \n",
    "# hyper-parameters etc. Also record to TensorBoard and write into csv, json\n",
    "class RunManager():\n",
    "  def __init__(self):\n",
    "\n",
    "    # tracking every epoch count, loss, accuracy, time\n",
    "    self.epoch_count = 0\n",
    "    self.epoch_loss = 0\n",
    "    self.epoch_num_correct = 0\n",
    "    self.epoch_start_time = None\n",
    "\n",
    "    # tracking every run count, run data, hyper-params used, time\n",
    "    self.run_params = None\n",
    "    self.run_count = 0\n",
    "    self.run_data = []\n",
    "    self.run_start_time = None\n",
    "\n",
    "    # record model, loader and TensorBoard \n",
    "    self.network = None\n",
    "    self.loader = None\n",
    "    self.tb = None\n",
    "\n",
    "  # record the count, hyper-param, model, loader of each run\n",
    "  # record sample images and network graph to TensorBoard  \n",
    "  def begin_run(self, run, network, loader):\n",
    "\n",
    "    self.run_start_time = time.time()\n",
    "\n",
    "    self.run_params = run\n",
    "    self.run_count += 1\n",
    "\n",
    "    self.network = network\n",
    "    self.loader = loader\n",
    "    self.tb = SummaryWriter(comment=f'-{run}')\n",
    "\n",
    "    images, labels = next(iter(self.loader))\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "    self.tb.add_image('images', grid)\n",
    "    self.tb.add_graph(self.network, images)\n",
    "\n",
    "  # when run ends, close TensorBoard, zero epoch count\n",
    "  def end_run(self):\n",
    "    self.tb.close()\n",
    "    self.epoch_count = 0\n",
    "\n",
    "  # zero epoch count, loss, accuracy, \n",
    "  def begin_epoch(self):\n",
    "    self.epoch_start_time = time.time()\n",
    "\n",
    "    self.epoch_count += 1\n",
    "    self.epoch_loss = 0\n",
    "    self.epoch_num_correct = 0\n",
    "\n",
    "  # \n",
    "  def end_epoch(self):\n",
    "    # calculate epoch duration and run duration(accumulate)\n",
    "    epoch_duration = time.time() - self.epoch_start_time\n",
    "    run_duration = time.time() - self.run_start_time\n",
    "\n",
    "    # record epoch loss and accuracy\n",
    "    loss = self.epoch_loss / len(self.loader.dataset)\n",
    "    accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
    "\n",
    "    # Record epoch loss and accuracy to TensorBoard \n",
    "    self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "    self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "\n",
    "    # Record params to TensorBoard\n",
    "    for name, param in self.network.named_parameters():\n",
    "      self.tb.add_histogram(name, param, self.epoch_count)\n",
    "      self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "    \n",
    "    # Write into 'results' (OrderedDict) for all run related data\n",
    "    results = OrderedDict()\n",
    "    results[\"run\"] = self.run_count\n",
    "    results[\"epoch\"] = self.epoch_count\n",
    "    results[\"loss\"] = loss\n",
    "    results[\"accuracy\"] = accuracy\n",
    "    results[\"epoch duration\"] = epoch_duration\n",
    "    results[\"run duration\"] = run_duration\n",
    "\n",
    "    # Record hyper-params into 'results'\n",
    "    for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "    self.run_data.append(results)\n",
    "    df = pd.DataFrame.from_dict(self.run_data, orient = 'columns')\n",
    "\n",
    "    # display epoch information and show progress\n",
    "    clear_output(wait=True)\n",
    "    display(df)\n",
    "\n",
    "  # accumulate loss of batch into entire epoch loss\n",
    "  def track_loss(self, loss):\n",
    "    # multiply batch size so variety of batch sizes can be compared\n",
    "    self.epoch_loss += loss.item() * self.loader.batch_size\n",
    "\n",
    "  # accumulate number of corrects of batch into entire epoch num_correct\n",
    "  def track_num_correct(self, preds, labels):\n",
    "    self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def _get_num_correct(self, preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "  \n",
    "  # save end results of all runs into csv, json for further a\n",
    "  def save(self, fileName):\n",
    "\n",
    "    pd.DataFrame.from_dict(\n",
    "        self.run_data, \n",
    "        orient = 'columns',\n",
    "    ).to_csv(f'{fileName}.csv')\n",
    "\n",
    "    with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "      json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all hyper params into a OrderedDict, easily expandable\n",
    "params = OrderedDict(\n",
    "    lr = [.01, .001],\n",
    "    batch_size = [100, 1000],\n",
    "    shuffle = [False,True]\n",
    ")\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = RunManager()\n",
    "\n",
    "# get all runs from params using RunBuilder class\n",
    "for run in RunBuilder.get_runs(params):\n",
    "\n",
    "    # if params changes, following line of code should reflect the changes too\n",
    "    network = Network()\n",
    "    loader = torch.utils.data.DataLoader(train_set, batch_size = run.batch_size)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
    "\n",
    "    m.begin_run(run, network, loader)\n",
    "    for epoch in range(epochs):\n",
    "      \n",
    "      m.begin_epoch()\n",
    "      for batch in loader:\n",
    "        \n",
    "        images = batch[0]\n",
    "        labels = batch[1]\n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        m.track_loss(loss)\n",
    "        m.track_num_correct(preds, labels)\n",
    "\n",
    "      m.end_epoch()\n",
    "    m.end_run()\n",
    "\n",
    "# when all runs are done, save results to files\n",
    "m.save('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to calculate all predictions of train set\n",
    "def get_all_preds(model, loader):\n",
    "  all_preds = torch.tensor([])\n",
    "  for batch in loader:\n",
    "    images, labels = batch\n",
    "\n",
    "    preds = model(images)\n",
    "    all_preds = torch.cat(\n",
    "        (all_preds, preds),\n",
    "        dim = 0\n",
    "    )\n",
    "  return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigger batch size since we only do FP\n",
    "prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=1000)\n",
    "train_preds = get_all_preds(network, prediction_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scikitplot to plot the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "\n",
    "cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(train_set.targets,train_preds.argmax(dim=1), normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Audio files from UCF101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ucf101 folder got it\n",
    "!mkdir ucf101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://crcv.ucf.edu/data/UCF101/UCF101.rar --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unrar e UCF101.rar data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir spec wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge librosa -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import librosa\n",
    "import glob as glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "def get_wavfilename(name):\n",
    "    name = name.split('.')[0]\n",
    "    name =  name.split('/')[1].split('_')[1:]\n",
    "    name = 'wav/'+'_'.join(name)+'.wav'\n",
    "    return name\n",
    "\n",
    "def gen_avitowav(src,des):\n",
    "    !ffmpeg -i $src -ab 160k -ac 2 -ar 44100 -vn $des -nostats -loglevel 0\n",
    "        \n",
    "# take ucf data\n",
    "ucf101 = (glob.glob(\"data/*\"))\n",
    "ucf101.sort()\n",
    "\n",
    "i =0 \n",
    "t = 0\n",
    "for avi in ucf101:\n",
    "    i+=1\n",
    "    if i%50==0:\n",
    "        print(i)\n",
    "        print(avi)\n",
    "    #generate wav file\n",
    "    wav = get_wavfilename(avi)\n",
    "    #!ffmpeg -i $vid -ab 160k -ac 2 -ar 44100 -vn $name -nostats -loglevel 0\n",
    "    gen_avitowav(avi,wav)\n",
    "    wav_dir = (glob.glob(\"wav/*\"))\n",
    "    size = len(wav_dir) \n",
    "    if size == t:\n",
    "        print(size,avi,wav)\n",
    "    t = size\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_wavtospec(src,des):\n",
    "    x , sr = librosa.load(src)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    #librosa.display.waveplot(x, sr=sr)\n",
    "    name = name.split('.')[0].split('/')[1]\n",
    "    name = 'spec/'+name+'.png'\n",
    "    plt.savefig(name, dpi=200)\n",
    "    \n",
    "def get_specfilename(name):\n",
    "    name = name.split('.')[0]\n",
    "    name =  name.split('/')[1].split('_')[1:]\n",
    "    name = 'spec/'+'_'.join(name)+'.png'\n",
    "    return name\n",
    "\n",
    "def get_wavfilename(name):\n",
    "    name = name.split('.')[0]\n",
    "    name =  name.split('/')[1].split('_')[1:]\n",
    "    name = 'wav/'+'_'.join(name)+'.wav'\n",
    "    return name\n",
    "\n",
    "def gen_avitowav(src,des):\n",
    "    !ffmpeg -i $src -ab 160k -ac 2 -ar 44100 -vn $des -nostats -loglevel 0\n",
    "\n",
    "\n",
    "wav =  get_wavfilename(glob.glob(\"wav/*\").sort()[0])\n",
    "\n",
    "#gen_avitowav(ucf101[0],wav)\n",
    "gen_specfilename(wav)\n",
    "spec = (glob.glob(\"spec/*\"))\n",
    "print(len(spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy\n",
    "import skimage.io\n",
    "\n",
    "def scale_minmax(X, min=0.0, max=1.0):\n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "def spectrogram_image(y, sr, out, hop_length, n_mels):\n",
    "    # use log-melspectrogram\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\n",
    "                                            n_fft=hop_length*2, hop_length=hop_length)\n",
    "    mels = numpy.log(mels + 1e-9) # add small number to avoid log(0)\n",
    "\n",
    "    # min-max scale to fit inside 8-bit range\n",
    "    img = scale_minmax(mels, 0, 255).astype(numpy.uint8)\n",
    "    img = numpy.flip(img, axis=0) # put low frequencies at the bottom in image\n",
    "    img = 255-img # invert. make black==more energy\n",
    "\n",
    "    # save as PNG\n",
    "    skimage.io.imsave(out, img)\n",
    "\n",
    "def get_specfilename(name):\n",
    "    name = name.split('.')[0]\n",
    "    name =  name.split('/')[1].split('_')\n",
    "    name = 'spec/'+'_'.join(name)+'.png'\n",
    "    return name\n",
    "\n",
    "def gen_wavtospec(src,des):\n",
    "    x , sr = librosa.load(src)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    #librosa.display.waveplot(x, sr=sr)\n",
    "    plt.savefig(des, dpi=200)\n",
    "\n",
    "# settings\n",
    "hop_length = 512 # number of samples per time-step in spectrogram\n",
    "n_mels = 128 # number of bins in spectrogram. Height of image\n",
    "time_steps = 384 # number of time-steps. Width of image\n",
    "    \n",
    "# take ucf data\n",
    "ucf101_wav = (glob.glob(\"wav/*\"))\n",
    "ucf101_wav.sort()\n",
    "\n",
    "i =0 \n",
    "t = 0\n",
    "for wav in ucf101_wav:\n",
    "    i+=1\n",
    "    if i%50==0:\n",
    "        print(i,wav)\n",
    "    \n",
    "    # load audio. Using example from librosa\n",
    "    path = wav\n",
    "    y, sr = librosa.load(path, offset=1.0, duration=10.0, sr=22050)\n",
    "    spec = get_specfilename(wav)\n",
    "\n",
    "    # extract a fixed length window\n",
    "    start_sample = 0 # starting at beginning\n",
    "    length_samples = time_steps*hop_length\n",
    "    window = y[start_sample:start_sample+length_samples]\n",
    "\n",
    "    # convert to PNG\n",
    "    spectrogram_image(window, sr=sr, out=spec, hop_length=hop_length, n_mels=n_mels)\n",
    "    print('wrote file', spec)\n",
    "    \n",
    "    \n",
    "    #generate spec file\n",
    "    #gen_wavtospec(wav,spec)\n",
    "    spec_dir = glob.glob(\"spec/*\")\n",
    "    size = len(spec_dir) \n",
    "    if size == t:\n",
    "        print(size,wav,spec)\n",
    "    t = size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_data =  glob.glob('spec/*_*_*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "name = glob.glob('wav/*')[0]\n",
    "\n",
    "window_size = 1024\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "y, sr = librosa.load(name)\n",
    "y = y[:100000] # shorten audio a bit for speed\n",
    "\n",
    "window = np.hanning(window_size)\n",
    "stft  = librosa.core.spectrum.stft(y, n_fft=window_size, hop_length=512, window=window)\n",
    "out = 2 * np.abs(stft) / np.sum(window)\n",
    "\n",
    "# For plotting headlessly\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "fig = plt.Figure()\n",
    "canvas = FigureCanvas(fig)\n",
    "ax = fig.add_subplot(111)\n",
    "p = librosa.display.specshow(librosa.amplitude_to_db(out, ref=np.max), ax=ax, y_axis='log', x_axis='time')\n",
    "fig.savefig('spec.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code segment converts ucf videos (avi) to audio (wav) to spectograms (png)\n",
    "import subprocess\n",
    "import librosa\n",
    "import glob as glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# take ucf data\n",
    "ucf101 = (glob.glob(\"data/*\"))\n",
    "ucf101.sort()\n",
    "\n",
    "i =0 \n",
    "t = 0\n",
    "for vid in ucf101:     \n",
    "  name = vid\n",
    "  #name = vid.split('.')[0]\n",
    "  i+=1\n",
    "  if i%50==0:\n",
    "    print(i)\n",
    "  #name =  name.split('/')[1].split('_')[1:]\n",
    "  #name = 'mp4/'+'_'.join(name)+'.mp4'\n",
    "  #name\n",
    "  #!ffmpeg -i $vid $name\n",
    "  #print(vid)\n",
    "  #generate wav file\n",
    "  #name = 'wav/'+'_'.join(name)+'.wav'  \n",
    "  #!ffmpeg -i $vid -ab 160k -ac 2 -ar 44100 -vn $name -nostats -loglevel 0\n",
    "  #generate spectogram\n",
    "  x , sr = librosa.load(name)\n",
    "  plt.figure(figsize=(14, 5))\n",
    "  #librosa.display.waveplot(x, sr=sr)\n",
    "  name = name.split('.')[0].split('/')[1]\n",
    "  name = 'spec/'+name+'.png'\n",
    "  plt.savefig(name, dpi=200)\n",
    "  spec = (glob.glob(\"spec/*\"))\n",
    "  size = len(spec) \n",
    "  if size == t:\n",
    "    print(size,name)\n",
    "  t = size\n",
    "  \n",
    "  \n",
    "  \n",
    "#subprocess.call(command, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goto f drive\n",
    "!cd /mnt/f/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/muhammadbsheikh/workspace/try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'wav/ApplyEyeMakeup_g01_c01.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will generate different variations of spectogram [medium link](https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8) \n",
    "*  chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "x , sr = librosa.load(path)\n",
    "print(type(x), type(sr))\n",
    "print(x.shape, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.axis('off')\n",
    "librosa.display.waveplot(x, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectograms/ sonographs/voiceprints/voicegrams\n",
    "# normal representation\n",
    "X = librosa.stft(x)\n",
    "Xdb = librosa.amplitude_to_db(abs(X))\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "plt.axis('off')\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logarithmic representation\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.axis('off')\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral centroid: It indicates where the ”centre of mass” for a sound is located and is calculated as the\n",
    "#weighted mean of the frequencies present in the sound.\n",
    "import sklearn\n",
    "\n",
    "spectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\n",
    "spectral_centroids.shape\n",
    "(775,)\n",
    "# Computing the time variable for visualization\n",
    "frames = range(len(spectral_centroids))\n",
    "t = librosa.frames_to_time(frames)\n",
    "# Normalising the spectral centroid for visualisation\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "#Plotting the Spectral Centroid along the waveform\n",
    "librosa.display.waveplot(x, sr=sr, alpha=0.4)\n",
    "plt.axis('off')\n",
    "plt.plot(t, normalize(spectral_centroids), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral Rolloff\n",
    "# It is a measure of the shape of the signal. It represents the frequency below which a specified \n",
    "#percentage of the total spectral energy, e.g. 85%, lies.\n",
    "spectral_rolloff = librosa.feature.spectral_rolloff(x+0.01, sr=sr)[0]\n",
    "librosa.display.waveplot(x, sr=sr, alpha=0.4)\n",
    "plt.axis('off')\n",
    "plt.plot(t, normalize(spectral_rolloff), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "#The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10–20) which concisely \n",
    "#describe the overall shape of a spectral envelope. It models the characteristics of the human voice.\n",
    "mfccs = librosa.feature.mfcc(x, sr=sr)\n",
    "print(mfccs.shape)\n",
    "#Displaying  the MFCCs:\n",
    "librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "# We can also perform feature scaling such that each coefficient dimension has zero mean and unit variance:\n",
    "import sklearn\n",
    "mfccs = sklearn.preprocessing.scale(mfccs, axis=1)\n",
    "print(mfccs.mean(axis=1))\n",
    "print(mfccs.var(axis=1))\n",
    "plt.axis('off')\n",
    "librosa.display.specshow(mfccs, sr=sr, x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma frequencies/Chromagram\n",
    "hop_length = 512\n",
    "chromagram = librosa.feature.chroma_stft(x, sr=sr, hop_length=hop_length)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.axis('off')\n",
    "librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "audio_files = glob.glob('wav/*')\n",
    "#audio = audio_files[0]\n",
    "\n",
    "for audio in audio_files:\n",
    "    #default sampling rate(sr) of 22KHZ mono\n",
    "    #sr=44100 for resampling\n",
    "    #load audio\n",
    "    x , sr = librosa.load(audio,sr=None)\n",
    "    #print(type(x), type(sr),x.shape, sr)\n",
    "\n",
    "    %matplotlib inline\n",
    "    #a6\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    mfccs = librosa.feature.mfcc(x, sr=sr)\n",
    "    #print(mfccs.shape)\n",
    "    #Displaying  the MFCCs:\n",
    "    librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
    "    plt.axis('off')\n",
    "    out = get_specfilename(audio,'a6/')\n",
    "    #out = 'a6/a6.png'\n",
    "    plt.savefig(out)\n",
    "    \n",
    "    \n",
    "    print(audio,len(glob.glob('a1/*')),len(glob.glob('a2/*')),len(glob.glob('a3/*')),len(glob.glob('a4/*')),\n",
    "         len(glob.glob('a5/*')),len(glob.glob('a6/*')),len(glob.glob('a7/*')),len(glob.glob('a8/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "print(len(glob.glob('a1/*')),len(glob.glob('a2/*')),len(glob.glob('a3/*')),len(glob.glob('a4/*')),\n",
    "         len(glob.glob('a5/*')),len(glob.glob('a6/*')),len(glob.glob('a7/*')),len(glob.glob('a8/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls a1/* a2/* a3/* a4/* a5/* a6/* a7/* a8/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "import pandas as pd\n",
    "cats = []\n",
    "audio_files = glob.glob('wav/*')\n",
    "for file in audio_files:\n",
    "    cats.append(file.split('/')[1].split('_')[0])\n",
    "\n",
    "df = pd.DataFrame(cats)\n",
    "print('Unique classes : '+str(df[0].nunique()))\n",
    "print(df[0].value_counts())\n",
    "print(len(df[0].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm a2/* a3/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm a1/* a2/* a3/* a4/* a5/* a6/* a7/* a8/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # a1 \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.axis('off')\n",
    "    librosa.display.waveplot(x, sr=sr)\n",
    "    # save as PNG\n",
    "    out = get_specfilename(audio,'a1/')\n",
    "    #out = 'a1/'+audio+''.png\n",
    "    plt.savefig(out)\n",
    "    #a2\n",
    "    X = librosa.stft(x)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.axis('off')\n",
    "    out = get_specfilename(audio,'a2/')\n",
    "    #out = 'a2/a2.png'\n",
    "    plt.savefig(out)\n",
    "    #a3\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.axis('off')\n",
    "    out = get_specfilename(audio,'a3/')\n",
    "    #out = 'a3/a3.png'\n",
    "    plt.savefig(out)\n",
    "    #a4\n",
    "    import sklearn\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\n",
    "    spectral_centroids.shape\n",
    "    (775,)\n",
    "    # Computing the time variable for visualization\n",
    "    frames = range(len(spectral_centroids))\n",
    "    t = librosa.frames_to_time(frames)\n",
    "    # Normalising the spectral centroid for visualisation\n",
    "    def normalize(x, axis=0):\n",
    "        return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "    #Plotting the Spectral Centroid along the waveform\n",
    "    librosa.display.waveplot(x, sr=sr, alpha=0.4)\n",
    "    plt.axis('off')\n",
    "    plt.plot(t, normalize(spectral_centroids), color='r')\n",
    "    out = get_specfilename(audio,'a4/')\n",
    "    #out = 'a4/a4.png'\n",
    "    plt.savefig(out)\n",
    "    #a5\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(x+0.01, sr=sr)[0]\n",
    "    librosa.display.waveplot(x, sr=sr, alpha=0.4)\n",
    "    plt.axis('off')\n",
    "    plt.plot(t, normalize(spectral_rolloff), color='r')\n",
    "    out = get_specfilename(audio,'a5/')\n",
    "    #out = 'a5/a5.png'\n",
    "    plt.savefig(out)\n",
    "    #a6\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    mfccs = librosa.feature.mfcc(x, sr=sr)\n",
    "    #print(mfccs.shape)\n",
    "    #Displaying  the MFCCs:\n",
    "    librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
    "    plt.axis('off')\n",
    "    out = get_specfilename(audio,'a6/')\n",
    "    #out = 'a6/a6.png'\n",
    "    plt.savefig(out)\n",
    "    #a7\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    mfccs = sklearn.preprocessing.scale(mfccs, axis=1)\n",
    "    #print(mfccs.mean(axis=1))\n",
    "    #print(mfccs.var(axis=1))\n",
    "    plt.axis('off')\n",
    "    librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
    "    out = get_specfilename(audio,'a7/')\n",
    "    #out = 'a7/a7.png'\n",
    "    plt.savefig(out)\n",
    "    #a8\n",
    "    hop_length = 512\n",
    "    chromagram = librosa.feature.chroma_stft(x, sr=sr, hop_length=hop_length)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.axis('off')\n",
    "    librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\n",
    "    out = get_specfilename(audio,'a8/')\n",
    "    #out = 'a8/a8.png'\n",
    "    plt.savefig(out)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fd88795326e6bc8ec526ae1a7e0f8acbdab9f97c60904165df3e5b2af06e756"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorchpy38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
