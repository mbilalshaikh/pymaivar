{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/muhammadbsheikh/workspace/try\n"
     ]
    }
   ],
   "source": [
    "cd /home/muhammadbsheikh/workspace/try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model,dl):    \n",
    "    model.eval()\n",
    "    device = 'cuda:0'\n",
    "    model.cuda(device)\n",
    "    with torch.no_grad():\n",
    "        features = []\n",
    "        for images,labels in tqdm(dl, disable=True):       \n",
    "            \n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            \n",
    "            #print(labels)\n",
    "\n",
    "            feat = output.data.cpu().numpy().copy(),labels\n",
    "            print(output.size(),labels)\n",
    "            features.append(feat)     \n",
    "            \n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def flatten_list(t):\n",
    "    flat_list = [item for sublist in t for item in sublist]\n",
    "    flat_list = np.array(flat_list)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "0.9.1\n",
      "_CudaDeviceProperties(name='GeForce GTX 1080 Ti', major=6, minor=1, total_memory=11178MB, multi_processor_count=28) None 0\n"
     ]
    }
   ],
   "source": [
    "rep = '8'\n",
    "# import standard PyTorch modules\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0,1,2,3\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pretrainedmodels\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter # TensorBoard support\n",
    "\n",
    "# import torchvision module to handle image manipulation\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# calculate train time, writing train data to files etc.\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import scipy\n",
    "import scipy.fft\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)     # On by default, leave it here for clarity\n",
    "\n",
    "\n",
    "#set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(torch.cuda.get_device_properties(device),torch.cuda.set_device(device),torch.cuda.current_device())\n",
    "\n",
    "#TRAIN_DATA_PATH = \"./datax/\"+rep+\"/train/\"\n",
    "#TEST_DATA_PATH = \"./datax/\"+rep+\"/test/\"\n",
    "TRAIN_DATA_PATH = \"/mnt/bdata/Data/bilal_data/ksounds_chrom/train\"\n",
    "TEST_DATA_PATH = \"/mnt/bdata/Data/bilal_data/ksounds_chrom/val\"\n",
    "\n",
    "transform = transforms.Compose([    \n",
    "    transforms.Resize(299), # preffered size for network\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=transform)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=False,  num_workers=4)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False,  num_workers=4)\n",
    "\n",
    "#prepare model\n",
    "model_name = 'inceptionresnetv2' # could be fbresnet152 or inceptionresnetv2\n",
    "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "model.last_linear = nn.Identity() #freeze the model\n",
    "#num_ftrs = model.last_linear.in_features\n",
    "#model.last_linear = nn.Linear(num_ftrs, 50)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad =False\n",
    "    \n",
    "PATH = \"models/IRv2.pt\"\n",
    "\n",
    "torch.save(model,PATH)\n",
    "#model = torch.load(PATH)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "def train(model,loader,epochs=60):\n",
    "    \n",
    "    model.train()   \n",
    "    print('Training...')\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        running_loss=0\n",
    "\n",
    "        for i,batch in enumerate(loader,0):\n",
    "                images = batch[0]\n",
    "                labels = batch[1]\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                model.to(device)\n",
    "                \n",
    "                preds = model(images)\n",
    "                loss = F.cross_entropy(preds, labels) # Adam, SGD, RSPROP\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = Variable(loss, requires_grad = True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                #running_loss+=loss.data\n",
    "                running_loss += loss.item()*images.size(0)\n",
    "\n",
    "                if i%9==0:\n",
    "                    end=time.time()\n",
    "                    #print ('[epoch %d,imgs %5d] time: %0.3f s'%(epoch+1,(i+1)*4,(end-start)))\n",
    "                    #print ('[epoch %d,imgs %5d] loss: %.7f  time: %0.3f s'%(epoch+1,(i+1)*128,running_loss/100,(end-start)))\n",
    "                    #tb.add_scalar('Loss', loss, epoch+1)\n",
    "                    start=time.time()\n",
    "        epoch_loss = running_loss / len(loader.dataset)\n",
    "        print(f'[epoch %d/%d] loss: %.3f '%(epoch,epochs,epoch_loss))\n",
    "    return model \n",
    "    \n",
    "\n",
    "#model = train(model,train_data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19349"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = extract_features(model,train_data_loader)\n",
    "test_features = extract_features(model,test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19349, 1344)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features),len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =0\n",
    "for x in train_data_loader.dataset.samples:\n",
    "    n = x[0].split('/')[4]+'/v_'+x[0].split('/')[5].split('.')[0]\n",
    "    with open('/home/muhammadbsheikh/workspace/projects/githubs_repos/tsn/tsn-pytorch/ucf_aud2vid_trainfiles1_new.txt') as f:\n",
    "        if n in f.read():\n",
    "            count=count+1\n",
    "        else:\n",
    "            print(n)\n",
    "print(count)\n",
    "count =0\n",
    "for x in test_data_loader.dataset.samples:\n",
    "    n = x[0].split('/')[4]+'/v_'+x[0].split('/')[5].split('.')[0]\n",
    "    with open('/home/muhammadbsheikh/workspace/projects/githubs_repos/tsn/tsn-pytorch/ucf_aud2vid_testfiles1_new.txt') as f:\n",
    "        if n in f.read():\n",
    "            count=count+1\n",
    "        else:\n",
    "            print(n)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "audio_train_features = [x[0] for x in train_features]\n",
    "audio_train_labels = [x[1] for x in train_features]\n",
    "np.save('/home/muhammadbsheikh/workspace/projects/githubs_repos/tsn/tsn-pytorch2/tsn-pytorch/features/audio_features-train'+rep+'.npy',np.array(audio_train_features))\n",
    "np.save('/home/muhammadbsheikh/workspace/projects/githubs_repos/tsn/tsn-pytorch2/tsn-pytorch/features/audio_labels-train'+rep+'.npy',np.array(audio_train_labels))\n",
    "#test\n",
    "audio_test_features = [x[0] for x in test_features]\n",
    "audio_test_labels = [x[1] for x in test_features]\n",
    "np.save('/home/muhammadbsheikh/workspace/projects/githubs_repos/tsn/tsn-pytorch2/tsn-pytorch/features/audio_features-test'+rep+'.npy',np.array(audio_test_features))\n",
    "np.save('/home/muhammadbsheikh/workspace/projects/githubs_repos/tsn/tsn-pytorch2/tsn-pytorch/features/audio_labels-test'+rep+'.npy',np.array(audio_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ksounds Feature Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 'ksounds'\n",
    "#train\n",
    "audio_train_features = [x[0] for x in train_features]\n",
    "audio_train_labels = [x[1] for x in train_features]\n",
    "np.save('/home/muhammadbsheikh/workspace/projects/ksounds_MAiVAR/features/audio_features-train'+rep+'.npy',np.array(audio_train_features))\n",
    "np.save('/home/muhammadbsheikh/workspace/projects/ksounds_MAiVAR/features/audio_labels-train'+rep+'.npy',np.array(audio_train_labels))\n",
    "#test\n",
    "audio_test_features = [x[0] for x in test_features]\n",
    "audio_test_labels = [x[1] for x in test_features]\n",
    "np.save('/home/muhammadbsheikh/workspace/projects/ksounds_MAiVAR/features/audio_features-test'+rep+'.npy',np.array(audio_test_features))\n",
    "np.save('/home/muhammadbsheikh/workspace/projects/ksounds_MAiVAR/features/audio_labels-test'+rep+'.npy',np.array(audio_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feat,lbls = extract_features(model,train_data_loader)\n",
    "test_feat,test_lbls = extract_features(model,test_data_loader)\n",
    "lbls = flatten_list(lbls)\n",
    "test_lbls  =flatten_list(test_lbls) # flatting the lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.shape,len(lbls),test_feat.shape,len(test_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Train-Without FFT')\n",
    "#SVM\n",
    "svm_preds = SVC(kernel='linear').fit(feat,lbls).predict(test_feat)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(test_lbls, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 #video index 4797\n",
    "j = 0 # neuron index 1000\n",
    "\n",
    "def hstft(S1):\n",
    "    S21, S22 = np.split(S1,2)  # 500+500\n",
    "    S311, S312 = np.split(S21,2) # 500    \n",
    "    S321, S322 = np.split(S22,2) # 500\n",
    "    #S1.shape,S21.shape,S22.shape,S311.shape,S312.shape,S321.shape,S322.shape    \n",
    "    return np.concatenate((S1,S21,S22,S311,S312,S321,S322))\n",
    "\n",
    "def alpha(features):\n",
    "    A = np.zeros((len(features),4608))\n",
    "    for i,f in enumerate(features):\n",
    "        h = scipy.fft.fft(f)    \n",
    "        #print(f.shape,h.shape)\n",
    "        #print(h.shape)\n",
    "        h = hstft(h)\n",
    "        #print(h.shape)\n",
    "        A[i,:] = h.real\n",
    "    return A\n",
    "\n",
    "train_alpha = alpha(feat)\n",
    "test_alpha = alpha(test_feat)\n",
    "train_alpha.shape,test_alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_alpha\n",
    "Y = lbls\n",
    "test_x = test_alpha\n",
    "test_y = test_lbls\n",
    "X.shape,Y.shape,test_x.shape,test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "#SVM\n",
    "clf = SVC(kernel='linear').fit(X,Y)\n",
    "preds = clf.predict(test_x)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(test_y, preds))\n",
    "\n",
    "#KNN\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3).fit(X,Y)\n",
    "knn_preds = knn_clf.predict(test_x)\n",
    "print(\"KNN Accuracy:\",metrics.accuracy_score(test_y, knn_preds))\n",
    "\n",
    "#Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100).fit(X,Y)\n",
    "rf_preds = rf_clf.predict(test_x)\n",
    "print(\"RF Accuracy:\",metrics.accuracy_score(test_y, rf_preds))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fd88795326e6bc8ec526ae1a7e0f8acbdab9f97c60904165df3e5b2af06e756"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorchpy38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
