{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "cd /home/muhammadbsheikh/workspace/try"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/muhammadbsheikh/workspace/try\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def extract_features(model,dl):\n",
    "    lbls = []\n",
    "    model.eval()\n",
    "    device = 'cuda:0'\n",
    "    model.cuda(device)\n",
    "    with torch.no_grad():\n",
    "        features = None\n",
    "        for batch in tqdm(dl, disable=True):\n",
    "            \n",
    "            images = batch[0]\n",
    "            labels = batch[1]\n",
    "            images = images.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            lbls.append(labels)\n",
    "            #print(labels)\n",
    "\n",
    "            if features is not None:\n",
    "                features = torch.cat((features, output), 0)\n",
    "\n",
    "            else:\n",
    "                features = output        \n",
    "            \n",
    "\n",
    "    return features.cpu().numpy(),lbls\n",
    "\n",
    "\n",
    "def flatten_list(t):\n",
    "    flat_list = [item for sublist in t for item in sublist]\n",
    "    flat_list = np.array(flat_list)\n",
    "    return flat_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# cnn2fft\n",
    "# check PyTorch versions\n",
    "# import standard PyTorch modules\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pretrainedmodels\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter # TensorBoard support\n",
    "\n",
    "# import torchvision module to handle image manipulation\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# calculate train time, writing train data to files etc.\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import scipy\n",
    "import scipy.fft\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "#device = torch.device(\"cuda\")\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)     # On by default, leave it here for clarity\n",
    "# torch.cuda.current_device()\n",
    "\n",
    "# Helper class, help track loss, accuracy, epoch time, run time, \n",
    "# hyper-parameters etc. Also record to TensorBoard and write into csv, json\n",
    "\n",
    "# import modules to build RunBuilder and RunManager helper classes\n",
    "from collections  import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "\n",
    "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
    "# combinations of hyper-parameters\n",
    "class RunBuilder():\n",
    "  @staticmethod\n",
    "  def get_runs(params):\n",
    "\n",
    "    Run = namedtuple('Run', params.keys())\n",
    "\n",
    "    runs = []\n",
    "    for v in product(*params.values()):\n",
    "      runs.append(Run(*v))\n",
    "    \n",
    "    return runs\n",
    "\n",
    "class RunManager():\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    # tracking every epoch count, loss, accuracy, time\n",
    "    self.epoch_count = 0\n",
    "    self.epoch_loss = 0\n",
    "    self.epoch_num_correct = 0\n",
    "    \n",
    "    self.epoch_val_loss = 0\n",
    "    self.epoch_val_num_correct = 0\n",
    "    self.epoch_start_time = None\n",
    "\n",
    "    # tracking every run count, run data, hyper-params used, time\n",
    "    self.run_params = None\n",
    "    self.run_count = 0\n",
    "    self.run_data = []\n",
    "    self.run_start_time = None\n",
    "\n",
    "    # record model, loader and TensorBoard \n",
    "    self.network = None\n",
    "    self.loader = None\n",
    "    self.vloader = None\n",
    "    self.tb = None\n",
    "\n",
    "  # record the count, hyper-param, model, loader of each run\n",
    "  # record sample images and network graph to TensorBoard  \n",
    "  def begin_run(self, run, network, loader,vloader):\n",
    "\n",
    "    self.run_start_time = time.time()\n",
    "\n",
    "    self.run_params = run\n",
    "    self.run_count += 1\n",
    "\n",
    "    self.network = network\n",
    "    self.loader = loader\n",
    "    self.vloader = vloader\n",
    "    self.tb = SummaryWriter(comment=f'-{run}')\n",
    "\n",
    "    images, labels = next(iter(self.loader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "    self.tb.add_image('images', grid)\n",
    "    self.tb.add_graph(self.network, images)\n",
    "\n",
    "  # when run ends, close TensorBoard, zero epoch count\n",
    "  def end_run(self):\n",
    "    self.tb.close()\n",
    "    self.epoch_count = 0\n",
    "\n",
    "  # zero epoch count, loss, accuracy, \n",
    "  def begin_epoch(self):\n",
    "    self.epoch_start_time = time.time()\n",
    "\n",
    "    self.epoch_count += 1\n",
    "    self.epoch_loss = 0\n",
    "    self.epoch_num_correct = 0\n",
    "\n",
    "  # \n",
    "  def end_epoch(self):\n",
    "    # calculate epoch duration and run duration(accumulate)\n",
    "    epoch_duration = time.time() - self.epoch_start_time\n",
    "    run_duration = time.time() - self.run_start_time\n",
    "\n",
    "    # record epoch loss and accuracy\n",
    "    loss = self.epoch_loss / len(self.loader.dataset)\n",
    "    accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
    "    \n",
    "    vloss = self.epoch_val_loss / len(self.vloader.dataset)\n",
    "    vaccuracy = self.epoch_val_num_correct / len(self.vloader.dataset)\n",
    "\n",
    "    # Record epoch loss and accuracy to TensorBoard \n",
    "    self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "    self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "    \n",
    "    # Record epoch loss and accuracy to TensorBoard \n",
    "    self.tb.add_scalar('vLoss', vloss, self.epoch_count)\n",
    "    self.tb.add_scalar('vAccuracy', vaccuracy, self.epoch_count)\n",
    "\n",
    "    # Record params to TensorBoard\n",
    "    for name, param in self.network.named_parameters():\n",
    "      self.tb.add_histogram(name, param, self.epoch_count)\n",
    "      self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "    \n",
    "    # Write into 'results' (OrderedDict) for all run related data\n",
    "    results = OrderedDict()\n",
    "    results[\"run\"] = self.run_count\n",
    "    results[\"epoch\"] = self.epoch_count\n",
    "    results[\"loss\"] = loss\n",
    "    results[\"val_loss\"] = self.epoch_val_loss\n",
    "    results[\"val_accuracy\"] = self.epoch_val_num_correct\n",
    "    results[\"accuracy\"] = accuracy\n",
    "    results[\"epoch duration\"] = epoch_duration\n",
    "    results[\"run duration\"] = run_duration\n",
    "\n",
    "    # Record hyper-params into 'results'\n",
    "    for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "    self.run_data.append(results)\n",
    "    df = pd.DataFrame.from_dict(self.run_data, orient = 'columns')\n",
    "\n",
    "    # display epoch information and show progress\n",
    "    clear_output(wait=True)\n",
    "    #display(df)\n",
    "\n",
    "  def track_vloss_vacc(self, loss, acc):\n",
    "    # multiply batch size so variety of batch sizes can be compared\n",
    "    self.epoch_val_loss = loss.item()\n",
    "    self.epoch_val_num_correct = acc\n",
    "    \n",
    "    \n",
    "    # accumulate loss of batch into entire epoch loss\n",
    "  def track_loss(self, loss):\n",
    "    # multiply batch size so variety of batch sizes can be compared\n",
    "    self.epoch_loss += loss.item() * self.loader.batch_size\n",
    "\n",
    "  # accumulate number of corrects of batch into entire epoch num_correct\n",
    "  def track_num_correct(self, preds, labels):\n",
    "    self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def _get_num_correct(self, preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "  \n",
    "  # save end results of all runs into csv, json for further a\n",
    "  def save(self, fileName):\n",
    "\n",
    "    pd.DataFrame.from_dict(\n",
    "        self.run_data, \n",
    "        orient = 'columns',\n",
    "    ).to_csv(f'{fileName}.csv')\n",
    "\n",
    "    with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "      json.dump(self.run_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "#set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(torch.cuda.get_device_properties(device),torch.cuda.set_device(device),torch.cuda.current_device())\n",
    "\n",
    "#Hyperparameters\n",
    "\n",
    "#input_size\n",
    "#num_classes = 50\n",
    "#learning_rate = 0.00005\n",
    "BATCH_SIZE = 32\n",
    "epochs = 1\n",
    "\n",
    "# put all hyper params into a OrderedDict, easily expandable\n",
    "params = OrderedDict(\n",
    "    exp='6',\n",
    "    lr = [0.0005],#,0.001,0.0001,0.00001], # 0.001\n",
    "    batch_size = [BATCH_SIZE], # 1000\n",
    "    shuffle = [True] # True,False\n",
    "    # Optimizer = [Adam,NAdam,RMSProp,Adamax,SGD,Adagrad,Adadelta]\n",
    ")\n",
    "\n",
    "TRAIN_DATA_PATH = \"./datax/6/train/\"\n",
    "TEST_DATA_PATH = \"./datax/6/test/\"\n",
    "\n",
    "transform = transforms.Compose([    \n",
    "    transforms.Resize(299), # preffered size for network\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True,  num_workers=4)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=True,  num_workers=4)\n",
    "\n",
    "m = RunManager()\n",
    "\n",
    "\n",
    "#prepare model\n",
    "model_name = 'inceptionresnetv2' # could be fbresnet152 or inceptionresnetv2\n",
    "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "\n",
    "model.last_linear = nn.Identity() #freeze the model\n",
    "#num_ftrs = model.last_linear.in_features\n",
    "#model.last_linear = nn.Linear(num_ftrs, 50)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad =False\n",
    "    \n",
    "#num_ftrs = model.last_linear.in_features\n",
    "\n",
    "    # Here the size of each output sample is set to 2.\n",
    "    # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "#model.fc = nn.Linear(num_ftrs, 50)\n",
    "\n",
    "PATH = \"models/IRv2.pt\"\n",
    "\n",
    "torch.save(model,PATH)\n",
    "#model = torch.load(PATH)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "def train(model,loader,epochs=60):\n",
    "    model.to(device)\n",
    "    model.train()   \n",
    "    print('Training...')\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        m.begin_epoch()\n",
    "        running_loss=0\n",
    "\n",
    "        for i,batch in enumerate(loader,0):\n",
    "                images = batch[0]\n",
    "                labels = batch[1]\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                preds = model(images)\n",
    "                loss = F.cross_entropy(preds, labels) # Adam, SGD, RSPROP\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                #loss = Variable(loss, requires_grad = True)\n",
    "                #loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss+=loss.data\n",
    "\n",
    "                if i%10==9:\n",
    "                    end=time.time()\n",
    "                    print ('[epoch %d,imgs %5d] time: %0.3f s'%(epoch+1,(i+1)*4,(end-start)))\n",
    "                    #print ('[epoch %d,imgs %5d] loss: %.7f  time: %0.3f s'%(epoch+1,(i+1)*4,running_loss/100,(end-start)))\n",
    "                    #tb.add_scalar('Loss', loss, epoch+1)\n",
    "                    start=time.time()\n",
    "                    running_loss=0    \n",
    "    \n",
    "\n",
    "train(model,train_data_loader)\n",
    "\n",
    "\n",
    "print('Extracting Features...')\n",
    "feat,lbls = extract_features(model,train_data_loader)\n",
    "# randomforest, logisticregression, SVM , KNN, LD,  \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.8.1\n",
      "0.9.1\n",
      "_CudaDeviceProperties(name='GeForce GTX 1080 Ti', major=6, minor=1, total_memory=11178MB, multi_processor_count=28) None 0\n",
      "Training...\n",
      "[epoch 1,imgs    40] time: 1.584 s\n",
      "[epoch 1,imgs    80] time: 0.963 s\n",
      "[epoch 1,imgs   120] time: 0.967 s\n",
      "[epoch 1,imgs   160] time: 0.966 s\n",
      "[epoch 1,imgs   200] time: 0.971 s\n",
      "[epoch 1,imgs   240] time: 0.967 s\n",
      "[epoch 1,imgs   280] time: 0.967 s\n",
      "[epoch 1,imgs   320] time: 0.965 s\n",
      "[epoch 1,imgs   360] time: 0.969 s\n",
      "[epoch 1,imgs   400] time: 0.971 s\n",
      "[epoch 1,imgs   440] time: 0.972 s\n",
      "[epoch 1,imgs   480] time: 0.972 s\n",
      "[epoch 1,imgs   520] time: 0.978 s\n",
      "[epoch 1,imgs   560] time: 0.970 s\n",
      "[epoch 1,imgs   600] time: 0.976 s\n",
      "[epoch 1,imgs   640] time: 0.971 s\n",
      "[epoch 1,imgs   680] time: 0.974 s\n",
      "[epoch 1,imgs   720] time: 0.973 s\n",
      "[epoch 1,imgs   760] time: 0.973 s\n",
      "[epoch 1,imgs   800] time: 0.973 s\n",
      "[epoch 1,imgs   840] time: 0.978 s\n",
      "[epoch 1,imgs   880] time: 0.974 s\n",
      "[epoch 1,imgs   920] time: 0.974 s\n",
      "[epoch 1,imgs   960] time: 0.977 s\n",
      "[epoch 1,imgs  1000] time: 0.977 s\n",
      "[epoch 1,imgs  1040] time: 0.976 s\n",
      "[epoch 1,imgs  1080] time: 0.980 s\n",
      "[epoch 1,imgs  1120] time: 0.975 s\n",
      "[epoch 1,imgs  1160] time: 0.978 s\n",
      "[epoch 1,imgs  1200] time: 0.979 s\n",
      "[epoch 2,imgs    40] time: 1.320 s\n",
      "[epoch 2,imgs    80] time: 0.979 s\n",
      "[epoch 2,imgs   120] time: 0.981 s\n",
      "[epoch 2,imgs   160] time: 0.981 s\n",
      "[epoch 2,imgs   200] time: 0.982 s\n",
      "[epoch 2,imgs   240] time: 0.982 s\n",
      "[epoch 2,imgs   280] time: 0.985 s\n",
      "[epoch 2,imgs   320] time: 0.979 s\n",
      "[epoch 2,imgs   360] time: 0.983 s\n",
      "[epoch 2,imgs   400] time: 0.982 s\n",
      "[epoch 2,imgs   440] time: 0.983 s\n",
      "[epoch 2,imgs   480] time: 0.983 s\n",
      "[epoch 2,imgs   520] time: 0.982 s\n",
      "[epoch 2,imgs   560] time: 0.983 s\n",
      "[epoch 2,imgs   600] time: 0.985 s\n",
      "[epoch 2,imgs   640] time: 0.982 s\n",
      "[epoch 2,imgs   680] time: 0.985 s\n",
      "[epoch 2,imgs   720] time: 0.985 s\n",
      "[epoch 2,imgs   760] time: 0.986 s\n",
      "[epoch 2,imgs   800] time: 0.985 s\n",
      "[epoch 2,imgs   840] time: 0.985 s\n",
      "[epoch 2,imgs   880] time: 0.986 s\n",
      "[epoch 2,imgs   920] time: 0.987 s\n",
      "[epoch 2,imgs   960] time: 0.987 s\n",
      "[epoch 2,imgs  1000] time: 0.985 s\n",
      "[epoch 2,imgs  1040] time: 0.987 s\n",
      "[epoch 2,imgs  1080] time: 0.986 s\n",
      "[epoch 2,imgs  1120] time: 0.986 s\n",
      "[epoch 2,imgs  1160] time: 0.987 s\n",
      "[epoch 2,imgs  1200] time: 0.988 s\n",
      "[epoch 3,imgs    40] time: 1.326 s\n",
      "[epoch 3,imgs    80] time: 0.991 s\n",
      "[epoch 3,imgs   120] time: 0.991 s\n",
      "[epoch 3,imgs   160] time: 0.990 s\n",
      "[epoch 3,imgs   200] time: 0.989 s\n",
      "[epoch 3,imgs   240] time: 0.991 s\n",
      "[epoch 3,imgs   280] time: 0.990 s\n",
      "[epoch 3,imgs   320] time: 0.990 s\n",
      "[epoch 3,imgs   360] time: 0.991 s\n",
      "[epoch 3,imgs   400] time: 0.990 s\n",
      "[epoch 3,imgs   440] time: 0.989 s\n",
      "[epoch 3,imgs   480] time: 0.991 s\n",
      "[epoch 3,imgs   520] time: 0.991 s\n",
      "[epoch 3,imgs   560] time: 0.992 s\n",
      "[epoch 3,imgs   600] time: 0.991 s\n",
      "[epoch 3,imgs   640] time: 0.992 s\n",
      "[epoch 3,imgs   680] time: 0.992 s\n",
      "[epoch 3,imgs   720] time: 0.990 s\n",
      "[epoch 3,imgs   760] time: 0.991 s\n",
      "[epoch 3,imgs   800] time: 0.993 s\n",
      "[epoch 3,imgs   840] time: 0.992 s\n",
      "[epoch 3,imgs   880] time: 0.993 s\n",
      "[epoch 3,imgs   920] time: 0.993 s\n",
      "[epoch 3,imgs   960] time: 0.994 s\n",
      "[epoch 3,imgs  1000] time: 0.994 s\n",
      "[epoch 3,imgs  1040] time: 0.992 s\n",
      "[epoch 3,imgs  1080] time: 0.993 s\n",
      "[epoch 3,imgs  1120] time: 0.995 s\n",
      "[epoch 3,imgs  1160] time: 0.990 s\n",
      "[epoch 3,imgs  1200] time: 0.993 s\n",
      "[epoch 4,imgs    40] time: 1.331 s\n",
      "[epoch 4,imgs    80] time: 0.993 s\n",
      "[epoch 4,imgs   120] time: 0.993 s\n",
      "[epoch 4,imgs   160] time: 0.993 s\n",
      "[epoch 4,imgs   200] time: 0.993 s\n",
      "[epoch 4,imgs   240] time: 0.994 s\n",
      "[epoch 4,imgs   280] time: 0.993 s\n",
      "[epoch 4,imgs   320] time: 0.994 s\n",
      "[epoch 4,imgs   360] time: 0.993 s\n",
      "[epoch 4,imgs   400] time: 0.994 s\n",
      "[epoch 4,imgs   440] time: 0.995 s\n",
      "[epoch 4,imgs   480] time: 0.994 s\n",
      "[epoch 4,imgs   520] time: 0.994 s\n",
      "[epoch 4,imgs   560] time: 0.994 s\n",
      "[epoch 4,imgs   600] time: 0.994 s\n",
      "[epoch 4,imgs   640] time: 0.994 s\n",
      "[epoch 4,imgs   680] time: 0.997 s\n",
      "[epoch 4,imgs   720] time: 0.992 s\n",
      "[epoch 4,imgs   760] time: 0.997 s\n",
      "[epoch 4,imgs   800] time: 0.993 s\n",
      "[epoch 4,imgs   840] time: 0.995 s\n",
      "[epoch 4,imgs   880] time: 0.994 s\n",
      "[epoch 4,imgs   920] time: 0.995 s\n",
      "[epoch 4,imgs   960] time: 0.995 s\n",
      "[epoch 4,imgs  1000] time: 0.995 s\n",
      "[epoch 4,imgs  1040] time: 0.995 s\n",
      "[epoch 4,imgs  1080] time: 0.996 s\n",
      "[epoch 4,imgs  1120] time: 0.999 s\n",
      "[epoch 4,imgs  1160] time: 0.996 s\n",
      "[epoch 4,imgs  1200] time: 0.994 s\n",
      "[epoch 5,imgs    40] time: 1.331 s\n",
      "[epoch 5,imgs    80] time: 0.996 s\n",
      "[epoch 5,imgs   120] time: 0.997 s\n",
      "[epoch 5,imgs   160] time: 0.995 s\n",
      "[epoch 5,imgs   200] time: 0.995 s\n",
      "[epoch 5,imgs   240] time: 0.997 s\n",
      "[epoch 5,imgs   280] time: 0.994 s\n",
      "[epoch 5,imgs   320] time: 0.997 s\n",
      "[epoch 5,imgs   360] time: 0.994 s\n",
      "[epoch 5,imgs   400] time: 0.999 s\n",
      "[epoch 5,imgs   440] time: 0.997 s\n",
      "[epoch 5,imgs   480] time: 0.995 s\n",
      "[epoch 5,imgs   520] time: 0.996 s\n",
      "[epoch 5,imgs   560] time: 0.996 s\n",
      "[epoch 5,imgs   600] time: 1.008 s\n",
      "[epoch 5,imgs   640] time: 0.993 s\n",
      "[epoch 5,imgs   680] time: 1.001 s\n",
      "[epoch 5,imgs   720] time: 1.002 s\n",
      "[epoch 5,imgs   760] time: 0.997 s\n",
      "[epoch 5,imgs   800] time: 0.998 s\n",
      "[epoch 5,imgs   840] time: 0.996 s\n",
      "[epoch 5,imgs   880] time: 0.997 s\n",
      "[epoch 5,imgs   920] time: 0.996 s\n",
      "[epoch 5,imgs   960] time: 0.998 s\n",
      "[epoch 5,imgs  1000] time: 0.997 s\n",
      "[epoch 5,imgs  1040] time: 0.997 s\n",
      "[epoch 5,imgs  1080] time: 0.997 s\n",
      "[epoch 5,imgs  1120] time: 0.998 s\n",
      "[epoch 5,imgs  1160] time: 0.998 s\n",
      "[epoch 5,imgs  1200] time: 0.997 s\n",
      "[epoch 6,imgs    40] time: 1.340 s\n",
      "[epoch 6,imgs    80] time: 0.995 s\n",
      "[epoch 6,imgs   120] time: 1.002 s\n",
      "[epoch 6,imgs   160] time: 0.992 s\n",
      "[epoch 6,imgs   200] time: 0.997 s\n",
      "[epoch 6,imgs   240] time: 0.999 s\n",
      "[epoch 6,imgs   280] time: 0.997 s\n",
      "[epoch 6,imgs   320] time: 0.997 s\n",
      "[epoch 6,imgs   360] time: 0.997 s\n",
      "[epoch 6,imgs   400] time: 0.997 s\n",
      "[epoch 6,imgs   440] time: 0.999 s\n",
      "[epoch 6,imgs   480] time: 0.998 s\n",
      "[epoch 6,imgs   520] time: 0.998 s\n",
      "[epoch 6,imgs   560] time: 0.995 s\n",
      "[epoch 6,imgs   600] time: 0.997 s\n",
      "[epoch 6,imgs   640] time: 0.997 s\n",
      "[epoch 6,imgs   680] time: 0.996 s\n",
      "[epoch 6,imgs   720] time: 0.997 s\n",
      "[epoch 6,imgs   760] time: 0.997 s\n",
      "[epoch 6,imgs   800] time: 0.998 s\n",
      "[epoch 6,imgs   840] time: 0.996 s\n",
      "[epoch 6,imgs   880] time: 0.997 s\n",
      "[epoch 6,imgs   920] time: 0.997 s\n",
      "[epoch 6,imgs   960] time: 0.997 s\n",
      "[epoch 6,imgs  1000] time: 0.997 s\n",
      "[epoch 6,imgs  1040] time: 0.997 s\n",
      "[epoch 6,imgs  1080] time: 1.001 s\n",
      "[epoch 6,imgs  1120] time: 1.001 s\n",
      "[epoch 6,imgs  1160] time: 0.996 s\n",
      "[epoch 6,imgs  1200] time: 0.997 s\n",
      "[epoch 7,imgs    40] time: 1.320 s\n",
      "[epoch 7,imgs    80] time: 0.998 s\n",
      "[epoch 7,imgs   120] time: 0.998 s\n",
      "[epoch 7,imgs   160] time: 1.003 s\n",
      "[epoch 7,imgs   200] time: 0.993 s\n",
      "[epoch 7,imgs   240] time: 0.998 s\n",
      "[epoch 7,imgs   280] time: 0.997 s\n",
      "[epoch 7,imgs   320] time: 0.998 s\n",
      "[epoch 7,imgs   360] time: 0.998 s\n",
      "[epoch 7,imgs   400] time: 0.997 s\n",
      "[epoch 7,imgs   440] time: 0.999 s\n",
      "[epoch 7,imgs   480] time: 0.999 s\n",
      "[epoch 7,imgs   520] time: 0.998 s\n",
      "[epoch 7,imgs   560] time: 0.997 s\n",
      "[epoch 7,imgs   600] time: 0.997 s\n",
      "[epoch 7,imgs   640] time: 0.998 s\n",
      "[epoch 7,imgs   680] time: 0.999 s\n",
      "[epoch 7,imgs   720] time: 0.997 s\n",
      "[epoch 7,imgs   760] time: 0.999 s\n",
      "[epoch 7,imgs   800] time: 0.997 s\n",
      "[epoch 7,imgs   840] time: 0.999 s\n",
      "[epoch 7,imgs   880] time: 0.998 s\n",
      "[epoch 7,imgs   920] time: 0.998 s\n",
      "[epoch 7,imgs   960] time: 0.998 s\n",
      "[epoch 7,imgs  1000] time: 0.999 s\n",
      "[epoch 7,imgs  1040] time: 0.999 s\n",
      "[epoch 7,imgs  1080] time: 0.997 s\n",
      "[epoch 7,imgs  1120] time: 1.000 s\n",
      "[epoch 7,imgs  1160] time: 0.998 s\n",
      "[epoch 7,imgs  1200] time: 0.999 s\n",
      "[epoch 8,imgs    40] time: 1.342 s\n",
      "[epoch 8,imgs    80] time: 0.997 s\n",
      "[epoch 8,imgs   120] time: 0.998 s\n",
      "[epoch 8,imgs   160] time: 0.999 s\n",
      "[epoch 8,imgs   200] time: 0.998 s\n",
      "[epoch 8,imgs   240] time: 0.995 s\n",
      "[epoch 8,imgs   280] time: 0.998 s\n",
      "[epoch 8,imgs   320] time: 0.998 s\n",
      "[epoch 8,imgs   360] time: 0.999 s\n",
      "[epoch 8,imgs   400] time: 1.000 s\n",
      "[epoch 8,imgs   440] time: 0.996 s\n",
      "[epoch 8,imgs   480] time: 0.998 s\n",
      "[epoch 8,imgs   520] time: 0.998 s\n",
      "[epoch 8,imgs   560] time: 0.999 s\n",
      "[epoch 8,imgs   600] time: 1.000 s\n",
      "[epoch 8,imgs   640] time: 0.996 s\n",
      "[epoch 8,imgs   680] time: 0.998 s\n",
      "[epoch 8,imgs   720] time: 0.998 s\n",
      "[epoch 8,imgs   760] time: 1.001 s\n",
      "[epoch 8,imgs   800] time: 0.997 s\n",
      "[epoch 8,imgs   840] time: 0.998 s\n",
      "[epoch 8,imgs   880] time: 0.999 s\n",
      "[epoch 8,imgs   920] time: 1.000 s\n",
      "[epoch 8,imgs   960] time: 0.999 s\n",
      "[epoch 8,imgs  1000] time: 0.999 s\n",
      "[epoch 8,imgs  1040] time: 0.998 s\n",
      "[epoch 8,imgs  1080] time: 0.998 s\n",
      "[epoch 8,imgs  1120] time: 0.999 s\n",
      "[epoch 8,imgs  1160] time: 0.998 s\n",
      "[epoch 8,imgs  1200] time: 0.999 s\n",
      "[epoch 9,imgs    40] time: 1.344 s\n",
      "[epoch 9,imgs    80] time: 0.999 s\n",
      "[epoch 9,imgs   120] time: 0.999 s\n",
      "[epoch 9,imgs   160] time: 0.998 s\n",
      "[epoch 9,imgs   200] time: 0.996 s\n",
      "[epoch 9,imgs   240] time: 0.999 s\n",
      "[epoch 9,imgs   280] time: 0.998 s\n",
      "[epoch 9,imgs   320] time: 0.999 s\n",
      "[epoch 9,imgs   360] time: 0.998 s\n",
      "[epoch 9,imgs   400] time: 0.999 s\n",
      "[epoch 9,imgs   440] time: 0.998 s\n",
      "[epoch 9,imgs   480] time: 0.999 s\n",
      "[epoch 9,imgs   520] time: 0.998 s\n",
      "[epoch 9,imgs   560] time: 0.998 s\n",
      "[epoch 9,imgs   600] time: 1.000 s\n",
      "[epoch 9,imgs   640] time: 0.999 s\n",
      "[epoch 9,imgs   680] time: 1.000 s\n",
      "[epoch 9,imgs   720] time: 0.997 s\n",
      "[epoch 9,imgs   760] time: 0.999 s\n",
      "[epoch 9,imgs   800] time: 1.001 s\n",
      "[epoch 9,imgs   840] time: 1.000 s\n",
      "[epoch 9,imgs   880] time: 1.000 s\n",
      "[epoch 9,imgs   920] time: 0.997 s\n",
      "[epoch 9,imgs   960] time: 0.999 s\n",
      "[epoch 9,imgs  1000] time: 0.998 s\n",
      "[epoch 9,imgs  1040] time: 1.001 s\n",
      "[epoch 9,imgs  1080] time: 0.997 s\n",
      "[epoch 9,imgs  1120] time: 0.998 s\n",
      "[epoch 9,imgs  1160] time: 0.998 s\n",
      "[epoch 9,imgs  1200] time: 1.000 s\n",
      "[epoch 10,imgs    40] time: 1.365 s\n",
      "[epoch 10,imgs    80] time: 0.998 s\n",
      "[epoch 10,imgs   120] time: 0.998 s\n",
      "[epoch 10,imgs   160] time: 0.998 s\n",
      "[epoch 10,imgs   200] time: 0.999 s\n",
      "[epoch 10,imgs   240] time: 0.999 s\n",
      "[epoch 10,imgs   280] time: 0.998 s\n",
      "[epoch 10,imgs   320] time: 1.001 s\n",
      "[epoch 10,imgs   360] time: 0.998 s\n",
      "[epoch 10,imgs   400] time: 1.002 s\n",
      "[epoch 10,imgs   440] time: 0.999 s\n",
      "[epoch 10,imgs   480] time: 0.998 s\n",
      "[epoch 10,imgs   520] time: 1.001 s\n",
      "[epoch 10,imgs   560] time: 0.997 s\n",
      "[epoch 10,imgs   600] time: 1.001 s\n",
      "[epoch 10,imgs   640] time: 0.996 s\n",
      "[epoch 10,imgs   680] time: 0.998 s\n",
      "[epoch 10,imgs   720] time: 0.999 s\n",
      "[epoch 10,imgs   760] time: 0.998 s\n",
      "[epoch 10,imgs   800] time: 0.999 s\n",
      "[epoch 10,imgs   840] time: 1.000 s\n",
      "[epoch 10,imgs   880] time: 0.999 s\n",
      "[epoch 10,imgs   920] time: 0.999 s\n",
      "[epoch 10,imgs   960] time: 0.997 s\n",
      "[epoch 10,imgs  1000] time: 0.999 s\n",
      "[epoch 10,imgs  1040] time: 0.999 s\n",
      "[epoch 10,imgs  1080] time: 1.000 s\n",
      "[epoch 10,imgs  1120] time: 0.998 s\n",
      "[epoch 10,imgs  1160] time: 1.000 s\n",
      "[epoch 10,imgs  1200] time: 0.999 s\n",
      "[epoch 11,imgs    40] time: 1.350 s\n",
      "[epoch 11,imgs    80] time: 1.000 s\n",
      "[epoch 11,imgs   120] time: 0.998 s\n",
      "[epoch 11,imgs   160] time: 0.998 s\n",
      "[epoch 11,imgs   200] time: 0.999 s\n",
      "[epoch 11,imgs   240] time: 1.001 s\n",
      "[epoch 11,imgs   280] time: 0.996 s\n",
      "[epoch 11,imgs   320] time: 0.999 s\n",
      "[epoch 11,imgs   360] time: 1.002 s\n",
      "[epoch 11,imgs   400] time: 0.997 s\n",
      "[epoch 11,imgs   440] time: 0.998 s\n",
      "[epoch 11,imgs   480] time: 0.998 s\n",
      "[epoch 11,imgs   520] time: 1.004 s\n",
      "[epoch 11,imgs   560] time: 0.996 s\n",
      "[epoch 11,imgs   600] time: 1.003 s\n",
      "[epoch 11,imgs   640] time: 1.001 s\n",
      "[epoch 11,imgs   680] time: 0.997 s\n",
      "[epoch 11,imgs   720] time: 0.999 s\n",
      "[epoch 11,imgs   760] time: 1.000 s\n",
      "[epoch 11,imgs   800] time: 0.998 s\n",
      "[epoch 11,imgs   840] time: 0.999 s\n",
      "[epoch 11,imgs   880] time: 1.000 s\n",
      "[epoch 11,imgs   920] time: 0.998 s\n",
      "[epoch 11,imgs   960] time: 1.001 s\n",
      "[epoch 11,imgs  1000] time: 0.999 s\n",
      "[epoch 11,imgs  1040] time: 0.997 s\n",
      "[epoch 11,imgs  1080] time: 0.999 s\n",
      "[epoch 11,imgs  1120] time: 0.999 s\n",
      "[epoch 11,imgs  1160] time: 1.006 s\n",
      "[epoch 11,imgs  1200] time: 0.994 s\n",
      "[epoch 12,imgs    40] time: 1.359 s\n",
      "[epoch 12,imgs    80] time: 0.997 s\n",
      "[epoch 12,imgs   120] time: 0.999 s\n",
      "[epoch 12,imgs   160] time: 0.999 s\n",
      "[epoch 12,imgs   200] time: 1.002 s\n",
      "[epoch 12,imgs   240] time: 0.996 s\n",
      "[epoch 12,imgs   280] time: 0.998 s\n",
      "[epoch 12,imgs   320] time: 1.000 s\n",
      "[epoch 12,imgs   360] time: 0.999 s\n",
      "[epoch 12,imgs   400] time: 0.999 s\n",
      "[epoch 12,imgs   440] time: 0.998 s\n",
      "[epoch 12,imgs   480] time: 0.997 s\n",
      "[epoch 12,imgs   520] time: 1.000 s\n",
      "[epoch 12,imgs   560] time: 1.001 s\n",
      "[epoch 12,imgs   600] time: 0.999 s\n",
      "[epoch 12,imgs   640] time: 0.999 s\n",
      "[epoch 12,imgs   680] time: 1.001 s\n",
      "[epoch 12,imgs   720] time: 0.999 s\n",
      "[epoch 12,imgs   760] time: 1.001 s\n",
      "[epoch 12,imgs   800] time: 0.999 s\n",
      "[epoch 12,imgs   840] time: 0.999 s\n",
      "[epoch 12,imgs   880] time: 1.001 s\n",
      "[epoch 12,imgs   920] time: 1.000 s\n",
      "[epoch 12,imgs   960] time: 1.003 s\n",
      "[epoch 12,imgs  1000] time: 0.999 s\n",
      "[epoch 12,imgs  1040] time: 0.998 s\n",
      "[epoch 12,imgs  1080] time: 1.001 s\n",
      "[epoch 12,imgs  1120] time: 0.997 s\n",
      "[epoch 12,imgs  1160] time: 0.997 s\n",
      "[epoch 12,imgs  1200] time: 0.999 s\n",
      "[epoch 13,imgs    40] time: 1.345 s\n",
      "[epoch 13,imgs    80] time: 0.998 s\n",
      "[epoch 13,imgs   120] time: 0.999 s\n",
      "[epoch 13,imgs   160] time: 0.998 s\n",
      "[epoch 13,imgs   200] time: 0.998 s\n",
      "[epoch 13,imgs   240] time: 1.003 s\n",
      "[epoch 13,imgs   280] time: 0.997 s\n",
      "[epoch 13,imgs   320] time: 1.001 s\n",
      "[epoch 13,imgs   360] time: 0.997 s\n",
      "[epoch 13,imgs   400] time: 0.997 s\n",
      "[epoch 13,imgs   440] time: 0.999 s\n",
      "[epoch 13,imgs   480] time: 0.999 s\n",
      "[epoch 13,imgs   520] time: 1.002 s\n",
      "[epoch 13,imgs   560] time: 0.996 s\n",
      "[epoch 13,imgs   600] time: 0.998 s\n",
      "[epoch 13,imgs   640] time: 0.999 s\n",
      "[epoch 13,imgs   680] time: 0.999 s\n",
      "[epoch 13,imgs   720] time: 0.999 s\n",
      "[epoch 13,imgs   760] time: 0.999 s\n",
      "[epoch 13,imgs   800] time: 0.997 s\n",
      "[epoch 13,imgs   840] time: 0.999 s\n",
      "[epoch 13,imgs   880] time: 1.000 s\n",
      "[epoch 13,imgs   920] time: 0.998 s\n",
      "[epoch 13,imgs   960] time: 0.998 s\n",
      "[epoch 13,imgs  1000] time: 0.998 s\n",
      "[epoch 13,imgs  1040] time: 0.999 s\n",
      "[epoch 13,imgs  1080] time: 1.000 s\n",
      "[epoch 13,imgs  1120] time: 0.998 s\n",
      "[epoch 13,imgs  1160] time: 0.998 s\n",
      "[epoch 13,imgs  1200] time: 0.999 s\n",
      "[epoch 14,imgs    40] time: 1.343 s\n",
      "[epoch 14,imgs    80] time: 1.000 s\n",
      "[epoch 14,imgs   120] time: 0.998 s\n",
      "[epoch 14,imgs   160] time: 0.998 s\n",
      "[epoch 14,imgs   200] time: 0.999 s\n",
      "[epoch 14,imgs   240] time: 1.000 s\n",
      "[epoch 14,imgs   280] time: 0.998 s\n",
      "[epoch 14,imgs   320] time: 1.002 s\n",
      "[epoch 14,imgs   360] time: 0.995 s\n",
      "[epoch 14,imgs   400] time: 1.000 s\n",
      "[epoch 14,imgs   440] time: 0.999 s\n",
      "[epoch 14,imgs   480] time: 1.000 s\n",
      "[epoch 14,imgs   520] time: 0.998 s\n",
      "[epoch 14,imgs   560] time: 0.999 s\n",
      "[epoch 14,imgs   600] time: 0.998 s\n",
      "[epoch 14,imgs   640] time: 1.002 s\n",
      "[epoch 14,imgs   680] time: 0.996 s\n",
      "[epoch 14,imgs   720] time: 0.998 s\n",
      "[epoch 14,imgs   760] time: 0.999 s\n",
      "[epoch 14,imgs   800] time: 1.000 s\n",
      "[epoch 14,imgs   840] time: 0.999 s\n",
      "[epoch 14,imgs   880] time: 1.000 s\n",
      "[epoch 14,imgs   920] time: 0.998 s\n",
      "[epoch 14,imgs   960] time: 1.000 s\n",
      "[epoch 14,imgs  1000] time: 1.000 s\n",
      "[epoch 14,imgs  1040] time: 1.001 s\n",
      "[epoch 14,imgs  1080] time: 0.999 s\n",
      "[epoch 14,imgs  1120] time: 0.997 s\n",
      "[epoch 14,imgs  1160] time: 0.999 s\n",
      "[epoch 14,imgs  1200] time: 1.000 s\n",
      "[epoch 15,imgs    40] time: 1.341 s\n",
      "[epoch 15,imgs    80] time: 1.001 s\n",
      "[epoch 15,imgs   120] time: 0.997 s\n",
      "[epoch 15,imgs   160] time: 1.000 s\n",
      "[epoch 15,imgs   200] time: 1.000 s\n",
      "[epoch 15,imgs   240] time: 0.997 s\n",
      "[epoch 15,imgs   280] time: 0.999 s\n",
      "[epoch 15,imgs   320] time: 1.000 s\n",
      "[epoch 15,imgs   360] time: 0.999 s\n",
      "[epoch 15,imgs   400] time: 1.000 s\n",
      "[epoch 15,imgs   440] time: 0.999 s\n",
      "[epoch 15,imgs   480] time: 0.998 s\n",
      "[epoch 15,imgs   520] time: 1.000 s\n",
      "[epoch 15,imgs   560] time: 0.998 s\n",
      "[epoch 15,imgs   600] time: 0.998 s\n",
      "[epoch 15,imgs   640] time: 1.000 s\n",
      "[epoch 15,imgs   680] time: 0.999 s\n",
      "[epoch 15,imgs   720] time: 0.999 s\n",
      "[epoch 15,imgs   760] time: 0.999 s\n",
      "[epoch 15,imgs   800] time: 0.999 s\n",
      "[epoch 15,imgs   840] time: 0.999 s\n",
      "[epoch 15,imgs   880] time: 1.000 s\n",
      "[epoch 15,imgs   920] time: 1.000 s\n",
      "[epoch 15,imgs   960] time: 0.997 s\n",
      "[epoch 15,imgs  1000] time: 1.000 s\n",
      "[epoch 15,imgs  1040] time: 0.999 s\n",
      "[epoch 15,imgs  1080] time: 1.000 s\n",
      "[epoch 15,imgs  1120] time: 0.999 s\n",
      "[epoch 15,imgs  1160] time: 0.999 s\n",
      "[epoch 15,imgs  1200] time: 1.000 s\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 178, in __getitem__\n    sample = self.loader(path)\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 215, in default_loader\n    return pil_loader(path)\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 195, in pil_loader\n    with open(path, 'rb') as f:\nFileNotFoundError: [Errno 2] No such file or directory: './datax/6/train/TableTennisShot/TableTennisShot_g16_c06.png'\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-414e3362f298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-414e3362f298>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, epochs)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mrunning_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 178, in __getitem__\n    sample = self.loader(path)\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 215, in default_loader\n    return pil_loader(path)\n  File \"/home/muhammadbsheikh/anaconda3/envs/pytorchpy38/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 195, in pil_loader\n    with open(path, 'rb') as f:\nFileNotFoundError: [Errno 2] No such file or directory: './datax/6/train/TableTennisShot/TableTennisShot_g16_c06.png'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "a = []\n",
    "for i,batch in enumerate(test_data_loader,0):\n",
    "    \n",
    "    lbl = batch[1]\n",
    "    a.append(lbl)\n",
    "    \n",
    "    if i%100==99:\n",
    "        break\n",
    "a = flatten_list(a)\n",
    "np_labels = np.array(a)\n",
    "print(np.min(np_labels),np.max(np_labels),np.mean(np_labels))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 49 24.74\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feat,lbls = extract_features(model,train_data_loader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "16*303"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4848"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "#feat,lbls = extract_features(model,train_data_loader)\n",
    "test_feat,test_lbls = extract_features(model,test_data_loader)\n",
    "#lbls = flatten_list(lbls)\n",
    "test_lbls  =flatten_list(test_lbls) # flatting the lbls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "len(test_feat),len(lbls),len(test_lbls)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3732, 4835, 3732)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_name = 'inceptionresnetv2' # could be fbresnet152 or inceptionresnetv2\n",
    "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet').cuda()\n",
    "\n",
    "for batch in train_data_loader:\n",
    "    out = res_extractor(batch[0].cuda())\n",
    "    print(out.shape)\n",
    "    break\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c3d_model = C3D()\n",
    "c3d_model.load_state_dict(torch.load('/home/muhammadbsheikh/workspace/c3d/one/c3d.pickle'))\n",
    "c3d_model.cuda()\n",
    "children_counter = 0\n",
    "for n,c in c3d_model.named_children():\n",
    "    print(\"Children Counter: \",children_counter,\" Layer Name: \",n,)\n",
    "    children_counter+=1\n",
    "c3d_model.modules"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c3ducf101_model = C3D(num_classes=101, pretrained=True)\n",
    "#c3ducf101_model.load_state_dict(torch.load('/home/muhammadbsheikh/workspace/projects/C3D jfzhang95/pytorch-video-recognition/run/run_2/models/C3D-ucf101_epoch-49.pth.tar'))\n",
    "\n",
    "checkpoint = torch.load('/home/muhammadbsheikh/workspace/projects/C3D jfzhang95/pytorch-video-recognition/run/run_2/models/C3D-ucf101_epoch-49.pth.tar')\n",
    "c3ducf101_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "children_counter = 0\n",
    "for n,c in c3ducf101_model.named_children():\n",
    "    print(\"Children Counter: \",children_counter,\" Layer Name: \",n,)\n",
    "    children_counter+=1\n",
    "c3ducf101_model.modules()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "c3ducf101_model.fc8 = nn.Identity()\n",
    "c3ducf101_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "C3D(\n",
       "  (conv1): Conv3d(3, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool2): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3a): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv3b): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4a): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv4b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5a): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv5b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool5): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
       "  (fc6): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "  (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (fc8): Identity()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "inputs = torch.rand(1, 3, 16, 112, 112)\n",
    "c3ducf101_model(inputs).size()\n",
    "from torchsummary import summary\n",
    "summary(c3ducf101_model)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "feat.shape,len(lbls),test_feat.shape,len(test_lbls)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((4835, 1536), 4835, (3732, 1536), 3732)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "trainpath = '/home/muhammadbsheikh/workspace/projects/mmaction/mmaction2/train_ucf50_feature.pkl'\n",
    "testpath = '/home/muhammadbsheikh/workspace/projects/mmaction/mmaction2/test_ucf50_feature.pkl'\n",
    "\n",
    "objects = []\n",
    "trainfile =  open(trainpath, \"rb\")\n",
    "testfile =  open(testpath, \"rb\")\n",
    "train_features = np.array(pickle.load(trainfile))\n",
    "test_features = np.array(pickle.load(testfile))\n",
    "        \n",
    "\n",
    "print(train_features.shape)\n",
    "print(test_features.shape)\n",
    "print(feat.shape)\n",
    "print(test_feat.shape)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4835, 2048)\n",
      "(1854, 2048)\n",
      "(4835, 1536)\n",
      "(3732, 1536)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "#concatenation\n",
    "cat_feat_train = np.concatenate((feat,train_features),axis=1)\n",
    "print(cat_feat_train.shape)\n",
    "cat_feat_test = np.concatenate((test_feat,test_features),axis=1)\n",
    "print(cat_feat_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4835, 3584)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3732 and the array at index 1 has size 1854",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-be2169ae63e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcat_feat_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_feat_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcat_feat_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_feat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_feat_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3732 and the array at index 1 has size 1854"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(np.min(train_features),np.max(train_features),np.mean(train_features))\n",
    "print(np.min(test_features),np.max(test_features),np.mean(test_features))\n",
    "print(np.min(feat),np.max(feat),np.mean(feat))\n",
    "print(np.min(test_feat),np.max(test_feat),np.mean(test_feat))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0 5.215954 0.36460656\n",
      "0.0 1422.8102 110.30214\n",
      "0.0 24.409187 0.26208088\n",
      "0.0 30.845383 0.26157546\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from sklearn import metrics\n",
    "print('Train-Without FFT')\n",
    "#SVM\n",
    "svm = SVC(kernel='linear').fit(cat_feat_train,lbls)\n",
    "svm_preds = svm.predict(cat_feat_test)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(test_lbls, svm_preds))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train-Without FFT\n",
      "SVM Accuracy: 0.016720604099244876\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "lbls.shape,test_lbls.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((4835,), (1854,))"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn import metrics\n",
    "print('Train-Without FFT')\n",
    "#SVM\n",
    "svm_preds = SVC(kernel='linear').fit(feat,lbls).predict(test_feat)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(test_lbls, svm_preds))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train-Without FFT\n",
      "SVM Accuracy: 0.46062567421790723\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#save np network features and labels\n",
    "# Use pickle for serialization or np.save for saving np objects. two options\n",
    "\n",
    "\n",
    "\n",
    "i = 0 #video index 4797\n",
    "j = 0 # neuron index 1000\n",
    "\n",
    "def hstft(S1):\n",
    "    S21, S22 = np.split(S1,2)  # 500+500\n",
    "    S311, S312 = np.split(S21,2) # 500    \n",
    "    S321, S322 = np.split(S22,2) # 500\n",
    "    #S1.shape,S21.shape,S22.shape,S311.shape,S312.shape,S321.shape,S322.shape    \n",
    "    return np.concatenate((S1,S21,S22,S311,S312,S321,S322))\n",
    "\n",
    "def alpha(features):\n",
    "    A = np.zeros((len(features),4608))\n",
    "    for i,f in enumerate(features):\n",
    "        h = scipy.fft.fft(f)    \n",
    "        #print(f.shape,h.shape)\n",
    "        #print(h.shape)\n",
    "        h = hstft(h)\n",
    "        #print(h.shape)\n",
    "        A[i,:] = h.real\n",
    "    return A\n",
    "\n",
    "train_alpha = alpha(feat)\n",
    "test_alpha = alpha(test_feat)\n",
    "train_alpha.shape,test_alpha.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((4835, 4608), (1854, 4608))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "X = train_alpha\n",
    "Y = lbls\n",
    "test_x = test_alpha\n",
    "test_y = test_lbls\n",
    "X.shape,Y.shape,test_x.shape,test_y.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((4835, 4608), (4835,), (1854, 4608), (1854,))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "#SVM\n",
    "clf = SVC(kernel='linear').fit(X,Y)\n",
    "preds = clf.predict(test_x)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(test_y, preds))\n",
    "\n",
    "#KNN\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3).fit(X,Y)\n",
    "knn_preds = knn_clf.predict(test_x)\n",
    "print(\"KNN Accuracy:\",metrics.accuracy_score(test_y, knn_preds))\n",
    "\n",
    "#Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100).fit(X,Y)\n",
    "rf_preds = rf_clf.predict(test_x)\n",
    "print(\"RF Accuracy:\",metrics.accuracy_score(test_y, rf_preds))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVM Accuracy: 0.4622437971952535\n",
      "KNN Accuracy: 0.3074433656957929\n",
      "RF Accuracy: 0.41585760517799353\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0239f972e8efcc00ec9fa7a595c90a818e30a75607d9e34132d6cc1dc8c76e26"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorchpy38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}